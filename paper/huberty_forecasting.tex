\documentclass{article}
\usepackage{natbib}
\usepackage[usenames, dvipsnames, svgnames, table]{xcolor}
\usepackage[dvipdfm,colorlinks=true,urlcolor=DarkBlue,linkcolor=DarkBlue,bookmarks=false,citecolor=DarkBlue]{hyperref}

\usepackage[pdftex]{graphicx}
\usepackage{fancyhdr}
\usepackage[T1]{fontenc}
\usepackage{palatino}
\usepackage[utf8]{inputenc}
%\usepackage[super]{nth}
\usepackage{setspace}
% \usepackage{placeins}
\usepackage{subfigure}
% \usepackage{multirow}
\usepackage{rotating}
\usepackage{marvosym}  % Used for euro symbols with \EUR
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\usepackage{longtable} %% Allows the use of the longtable format produced by xl2latex.rb
\usepackage{lscape} %% Allows landscape orientation of tables
% \usepackage{appendix} %% Allows customization of the appendix properties
\setcounter{tocdepth}{1} %% Restricts the table of contents to the section header level entries only


\usepackage{geometry}
\geometry{letterpaper}
\usepackage{amsmath}


\begin{document}
\title{Can we vote with our tweet?\\ On the perennial difficulty of election
forecasting with social media}
\author{
Mark Huberty\thanks{Enormous credit and thanks are due to Len DeGroot of the
  Graduate School of Journalism at the University of California,
  Berkeley for hosting real-time publication of predictions during the
2012 election; and to Hillary Sanders for invaluable research
support. Earlier versions of this paper benefited from commends from F. Daniel Hidalgo, Jasjeet Sekhon, and
participants at the 2011 UC Berkeley Research Workshop in American
politics for helpful comments and feedback. Chris Diehl, Dave
Gutelius, and Sean Taylor all provided valuable sounding boards for
thinking about the forecasting problem more broadly. The usual disclaimers apply.}
}
% \thanks{This version prepared for the Midwest
%     Political Science Association Conference, April 2013, Chicago. First version: February 2011. Special
%     thanks to the Graduate School of Journalism at the University of
%     California, Berkeley for hosting the Voting with your Tweets
%     experiment for the 2012 Congressional elections. This project
%     would not have come off without the support, input, and hard work
%     of Len DeGroot and Hillary Sanders. Additional thanks to
%     to F. Daniel Hidalgo, Jasjeet
%     Sekhon, and participants at the 2011 Society for Political
%     Methodology meeting, and the Fall 2011 UC Berkeley Research Workshop in
%     American Politics, for helpful comments and feedback. All errors remain my
%     own.}

% \author{Mark Huberty\thanks{Travers Department of Political Science,
%     University of California, Berkeley. Contact:
%     \url{markhuberty@berkeley.edu}.}\\ PRELIMINARY DRAFT}
% \date{\today}
\maketitle
\begin{abstract}

\end{abstract}

\doublespacing
\section{Introduction}
\label{sec:introduction}

%% Point: discuss social media forecasting as a historical
%% phenomenon. Point to failures. Discuss importance of
%% baselines. Then focus on Twitter as a valid measure. Point to
%% instability across elections. Point to weirdness of sample and
%% sample dynamics year-over-year / election-over-election. Discuss
%% import of instability. Discuss import of gaming, systemic problems
%% of influence (e.g., much less sampling control relative to polling,
%% so models will be much more important.)Close
%% with recs. Openness among them. Point to Gayo-Avello as a major
%% thing. 

Virtualization of human behavior has thrown off enormous quantities of
data about what people do or say, when whom, and when. The presence of
such data has naturally given rise to the desire to study past behavior and,
ultimately, forecast the future. High-profile commercial successes in
advertising, sales, logistics, and other fields hint that this desire
may be fulfilled. If so, it may offer salvation to political
professionals and scholars struggling with the increasing cost and
difficulty of traditional survey research and electoral polling.

Yet experience to date in forecasting important events, political or
otherwise, has not always borne out the promise of new predictive
powers. Success in predicting what people want to buy, or what ads
they might click on, has not translated into reliable success in
sectors such as public health or finance. Research into other means of
online opinion discovery, such as consumer rating systems, has shown
them vulnerable to systematic bias. These difficulties have cast doubt
on the potential for social media data to provide a new means of
forecasting high-profile real-world events.

This paper discusses the history of electoral forecasting from social
media and other online data. I note that all
known examples of apparently successful forecasts have very quickly
encountered problems that have undermined their predictive
power. Initial success at beating simple heuristics for electoral
success, such as incumbency, has faded quickly when faced with the
challenge of forward-looking election prediction. These difficulties have led to a short
shelf life for  high-profile claims that social media data
will soon supplant polling. Hence while
social media and other online data may be useful for studying how
citizens behave online, it has proven rather less useful for
forecasting what those same citizens--and, more importantly, their
offline fellows--will do back in the real world.

I conclude by discussing whether these problems can be overcome. I
agree with earlier critiques of the election forecasting literature, which
recommended fundamental changes in transparency, disclosure, method, and
publication of electoral forecasts that rely on social media. But I
raise the question of whether we should ever expect reliable election
forecasts even given these changes. Social media forecasting, because
it lacks the control over the data-generating process enjoyed by
polling, faces enormous instability in the origin, sample, structure,
and content of the data on which it relies. That instability appears
severe enough to undermine forecasting algorithms even in systems
short election cycles. Absent much more sophisticated methods for
discovering and correcting for this instability, it's not clear why we
should expect forecasting from social media to work at all. 



% \section{The internet as instrument for human behavior}
% \label{sec:intern-as-instr}

% From a data perspective, the internet is simultaneously two
% things. First and foremost, it is a catalogue of human social
% structures. By now, the internet expresses most of humanity's wants,
% needs, desires, behaviors, languages, and social patterns. Given
% global patterns of wealth and access, it may represent these things in
% proportion to how they really occur. But represent them it does. 

% Secondarily, it is an instrument to detect how people
% interact with that catalogue and with others who use it. At the
% boundary between the offline and online world, the internet throws off
% huge amounts of data on what people look for and at, in combination with what
% other things, what they say about it, and to whom. 

% Heretofore, perhaps the most productive uses of the online world to
% measure and forecast the offline have focused on the former
% category. The internet as a catalogue of human language has anchored
% rapid advances in automated translation. The internet as a catalogue
% of knowledge supported IBM's successful bid to win Jeopardy with a
% computer. Both achievements relied on a catalogue of knowledge novel
% in its breadth, depth, and accessibility. 


% % Note here that the very successful uses of the
% % internet for offline behavior--translation, etc--rely on the first
% % version. 


% \section{Forecasting: through the glass, darkly}
% \label{sec:forec-thro-glass}

% Yet the internet as instrument holds perhaps the greater promise.  It
% does so as testament to the firms that made early use of the ability
% to detect and exploit human behavior. The companies operating the
% instruments, faced with the need to earn money for providing what
% quickly became public goods, turned to this data as a revenue
% source. But they did not simply sell the data. Rather, they used the
% data to act as intermediaries between those who wanted it--mostly
% companies interested in what their customers wanted--and users. What
% began with Google is now a ubiquitous feature of any large-scale
% internet services firm: advertising narrowly targeted to the user
% based on the data the firm gathers on that user in the course their of
% using its services. That model has proven so successful that it has,
% on the one hand, generated an entirely new economy; and, on the other,
% almost destroyed the older advertising model that supported newspapers
% and magazines. Online data put in service of predicting online
% behaviors and interests turned out to have enormous potential, and
% with it enormous disruptive power.

% To date, however, that potential has not extended to predicting
% offline behavior. The reasons why provide an instructive entry into
% the discussion of election forecasting from social media. Forecasting
% online behavior from online data enjoyed a much closer correspondence
% between historical data and future actions. It also benefited from
% much weaker consequences from predictive failure. In contrast, where
% online data has failed to predict offline behavior, that failure has
% come with large potential or real consequences. 


% \subsection{Success: targeting the person}
% \label{sec:targeting-person}

% % point here: that what's worked has been self-referential in large
% % part. Predict online behavior from online data. 

% As of this writing, Google's AdSense is now ten years
% old.\footnote{Jeff Dean, one of Google's early engineers, recounts its
% early days here:
% \url{http://googleblog.blogspot.com/2013/06/celebrating-10-years-of-shared-success.html}.}
% Together with its advertiser-side counterpart AdWords, it
% revolutionized how both publishers and advertisers bought and sold
% online advertising. The premise was straightforward: by using
% sophisticated algorithms, Google could narrowly categorize the content
% on any given website and serve ads narrowly tailored to that
% content. If newspapers once put fashion ads in the style section,
% Google could narrowly choose which ad to serve for each
% paragraph. More narrowly targeted ads proved more effective for sellers
% and less annoying for buyers. By forecasting offline interests through
% online internet behavior, AdWords and AdSense forever changed the
% advertising business. Google today earns tens of billions of dollars
% annually based on this success.

% Other companies found similar success. LinkedIn's ``people you may
% know'' product proved surprisingly accurate at finding long-lost
% colleagues in their database of professionals. Facebook's FriendFinder
% did the same for long-lost social connections. Twitter uses content
% and network algorithms to recommend other users of interest. Amazon
% very effectively detects both product substitutes and complements and
% recommends them to potential buyers. Target, in what was
% simultaneously an analytics coup but a PR nightmare, figured out that
% a teenage girl was pregnant before her parents did
% \citep{hill2012}. In these and other cases, the internet as an
% instrument of the offline world proved a fertile ground for
% forecasting social structure and behavior.

% \subsection{Failure: predicting the offline world}
% \label{sec:fail-pred-offl}


% These successes in hand, they suggested the possibility of more
% ambitious forecasts. If users increasingly studied or expounded on the
% offline world in online tools, then perhaps that data could provide
% the foundation for offline as well as online prediction. Furthermore,
% while these early successes focused on largely commercial
% behavior--advertising, purchasing, professional networking--ambitions
% quickly moved beyond that to larger social and economic problems.

% To date, however, performance has fallen short of these ambitions. Two
% instructive examples help to understand why. Both attempted to
% forecast complex real-world phenomena from online social and search
% data. Both proceeded from reasonable evidence that they might
% work. And yet both failed at important junctures--and failed in ways
% that were very hard to identify ahead of time. 

% \subsubsection{Google Flu}
% \label{sec:google-flu}


% We begin with Google Flu.\footnote{See
%   \url{http://www.google.org/flutrends/us/\#US} for the latest
%   forecasts.} The Flu project began from a simple hypothesis: when
% people get sick with the flu, they go to Google and search for
% specific things. If Google could identify what those things were, and
% track the frequency of related searches, then perhaps it could
% accurately forecast rates of influenza infection. In 2006-2007, when
% the project began, real-time flu infection
% forecasts marked a substantial gain over Centers for Disease Control
% systems, whose influenza survey data arrived with a two week lag. 

% As \cite{ginsberg2008detecting} reported, this hypothesis proved
% correct. Google's Flu team could accurately forecast
% geographically-specific rates of influenza infections in the US, in
% real time. To do this, Google mined millions of search queries from
% its historic database, comparing their frequency with historical CDC
% influenza infection data. The resulting model performed extremely well
% in out-of-sample historical tests. 

% Yet Google Flu has since failed twice, both times in the midst of
% exceptional--and thus more problematic--flu seasons. It first failed
% during the 2009 H1N1 flu outbreak, when it under-forecast influenza
% rates. It failed again in 2012, when it over-forecast influenza in an
% early-onset flu season. Under the very exceptional circumstances in
% which public health authorities might want improved monitoring tools,
% Google Flu broke down. 

% The reasons for the breakdown are instructive. In 2009, the viral
% character of H1N1 meant that individuals didn't know they had the
% flu. Hence their search behavior didn't reflect a search for flu
% remedies; and if it did, it used novel terms, reflecting a novel
% strain, not present in the Google Flu lexicon. In 2012, public health
% authorities worried about an early flu season were very proactive in
% getting flu information out. The public, primed by this publicity
% campaign, went looking for flu information--but did so while still healthy
% \citep{butler2013google}.

% These failures thus both embody a break between Google Flu's 
% hypothesis linking offline and online behavior. The link was
% plausible: people get sick, and go looking to Google for
% information. But in 2009 and 2012, the link broke. It broke first
% because people didn't know they were sick with the flu, and so didn't
% go looking; and then later, because people went looking when they
% weren't sick. But there was no way to know the link was broken until
% after the fact, when the online predictions could be checked against
% more traditional clinical surveys still used by the CDC. Nothing about
% the model or the method permitted real-time checking of its most
% fundamental assumptions. While Google has since updated its models to
% reduce their sensitivity to outlier terms \citep{41763}, this
% underlying problem remains unsolved.

% \subsubsection{Twitter-based hedge funds}
% \label{sec:twitter-based-hedge}

% Finance has provided a second test of the online-predicts-offline
% thesis. The utility of real-time data on important events has driven
% finance at least since Rothschild was purported to have deployed
% carrier pigeons to transmit the outcome of the battle at Waterloo to
% his trading desk in London. 

% Hedge funds moved to capitalize on Twitter as the newest source of
% information arbitrage. Starting around 2009, several papers suggested
% that sentiment analysis of the Twitter feed could accurately forecast
% stock market aggregates
% \citep{bollen2011twitter,zhang2011predicting}. In 2011, 
% Derwent Capital Management put upwards of \$40 million into a hedge
% fund built around social media analysis. That fund closed within a
% month. More recent research still suggests that Twitter may have some
% informative value for financial analysts, but cautions against the
% optimism of early papers \citep{mackintosh2013}. Other real-world
% experiments in trading on Twitter data have found the signal too noisy
% and incoherent to have much effect. Meanwhile, October
% 2013 saw an errant crash as oil traders reacted to tweets about a war in
% the Middle East. Only later did they realize that those tweets
% commemorated the anniversary of the 1973 Yom Kippur War. 

% The shortcomings of Twitter as a stock market forecast share much in
% common with Google Flu. Here, as there, forecasts assumed a specific
% relationship between real-world action and online expression. Here,
% the relationship was far less well specified. Google Flu had a very
% clear idea of the causal chain from infection to search. Twitter as a
% stock market forecast did not: instead, it attempts to mine a weak
% connection between sentiments expressed towards a particular brand or
% company, and its future performance on the exchanges. As in Google
% Flu, the models at stake had no means of validating \textit{a priori}
% whether this hypothesis held. Instead, they had to wait for actual
% returns to roll in. To date, very little evidence suggests that
% Twitter sentiment provided a meaningful real-world edge. Moreover, if
% it had, we should expect to see such an edge quickly arbitraged away.
%% Expand this point.

% Flu
% Twitter hedge fund

% Characteristics: little introspection into DGP. Hypothesis about
% link between offline and online behavior. Can't check the hypo until
% we know if the forecast succeeds or fails. Op for strategic behavior
% renders hypo permanently vulnerable to failure, not just a matter of
% collecting more data. 

\section{Forecasting the political}
\label{sec:forec-polit}

% Point out tea party, obama as harbingers of things to come. 
% Lots of claims. Then lots of failures. 

From what began as a catalogue of human knowledge, the internet has
become an instrument for monitoring human behavior. Using the data
thrown off by that instrument, new companies have successfully
measured and predicted a range of online and offline behaviors. The
most notable successes include radically more effective ad targeting,
translation, 
and email spam detection
(Google), discovery and recommendation of professional (LinkedIn) and
social (Facebook) contacts, early detection of and marketing to people
with specific medical conditions like pregnancy \citep{hill2012},
and real-time measurement of geographically-specific rates of influenza
\citep{ginsberg2008detecting}. Some of these early successes (notably Google
Flu, see \cite{butler2013google} and \cite{41763}) have later encountered
difficulties. But the potential for predicting the future from 
traces of the past left behind on servers around the world remains.

Political forecasting from social and online data gained currency in
parallel with, and perhaps prompted by, these successes. Apart from
sheer novelty, several forces conspired to push scholars and political
professionals to pursue this problem. The importance of online media grew after
2008: President Barack Obama's digital effort was widely hailed in
both 2008 and 2012
\citep{smith2009internet,levenshus2010online,wallsten2010yes,scherer2012}; and the
Tea Party insurgency that brought the Republican party back into
control of the U.S. House of Representatives relied initially on
online social media to circumvent traditional party channels
\citep{williamson2011tea}. In parallel, traditional public opinion
surveys began to struggle with rising non-response rates and increased
cell phone usage, prompting a search for new means of measuring and
forecasting political attitudes and intent
\citep{keeter2006gauging,christian2010assessing,kohut2012assessing,boyle2013sampling,viera2013mail}.

Social media services were among the most tempting
targets. 2010 say a way of papers of the form ``using
<social media source X> to predict political contest Y'' began to
arrive, scattered across computer science, machine learning, and
social science journals.\footnote{X = Twitter in most of these papers, with a few notable
  exceptions \citep{bond201261,broockman2013online}. Unlike the other major online data companies, Twitter has made
  its data relatively freely available (albeit with heavy restrictions
  on subsequent distribution). Among other options, Facebook appears open to
  social science research but requires more up-front investment in
  relationship building. Google is largely closed, though aggregated
  data from Google Trends are freely available. Hence Twitter-based
  forecasting faces far lower startup costs than the alternatives. The reasons for these
  differences, and their consequences for patterns of social science
  research, deserve their own paper.}
These papers often promised more than simply an
alternative to traditional polling. Social media users appeared oddly
insensitive to the usual social mores that inhibit survey respondents from expressing
controversial or taboo opinions. If this insensitivity held at scale, then social
media also promised a solution to the longstanding
difficulty of measuring individuals' true attitudes towards taboo
subjects like racism or homophobia.\footnote{Whether social media are any less prone to this
is an open question. We might all be surprised at the blatant racism
displayed by many individuals online. But of course this raises the
same question: are these individuals in fact this racist in real life?
Or do they merely express such racism as a matter of conforming to the
social norms of their online community? \cite{wilson2012review}
review research suggesting that users' behavior on Facebook may in
fact reflect their true selves. On the other hand, internet comment
threads are famously divisive unless heavily policed \citep{binns2012don}.}  While someone might not tell a phone survey worker that they
voted against Barack Obama because he was black, many internet users
users appeared to have no such qualms about broadcasting the
same.\footnote{\cite{stephens2013cost} exploits this hypothesis to
  estimate that racial animus cost Barack Obama 3-5\% of the national
  vote. Hard verification of this estimate remains difficult for the
  very reason that this estimate is valuable: the inability to
  benchmark it to survey data of actual human subjects.}

Unfortunately, political forecasting has far less success predicting
the offline world from online behavior than its commercial
predecessors. All known attempts at election forecasting with social
media have failed. Some have failed simply because models that tested
well on contemporaneous out-of-sample data performed poorly on future
elections. Others have failed because their initial success relied on
undisclosed data manipulation on the part of researchers.

Here we present a selection of attempts and failures to forecast
elections using the Twitter social media service. We discuss in detail
our own experience in what we believe to be the first attempt at
multi-cycle forecasting of U.S. House of Representatives
elections. There, as in other attempts, promising forecasting methods
failed to beat simple heuristics like incumbency in predicting
election outcomes. Close examination of the underlying data suggests
why: both changes to the political landscape and to the demographics
and behavior of the Twitter user community altered the data-generating
process in ways the models could not account for. By implication,
successful forecasting models must have the capability to dynamically
check and update their assumptions about the real world. 

% \subsection{Twitter as political phenomenon}
% \label{sec:twitter-as-political}



% Want a brief thing here on what we might measure. (1) sentiment of
% individual messages, (2) sentiment of individual users, (3) partisan
% alignment of individual users, (4) changes in partisan alignment of
% individual users. These are all screwy

\subsection{The universal failure of election forecasts}
\label{sec:univ-fail-elect}

Daniel Gayo-Avello and coauthors \citep{gayo2011limits,metaxas2011not}
provide a long catalog of failed attempts at election forecasting with
Twitter. Here we recount a few notable examples. \cite{o2010tweets}
led the pack in political forecasting with an analysis of Twitter and
the 2008 presidential polls. They proposed a simple model wherein the
share of mentions for John McCain or Barack Obama, and the sentiment
attached to those mentions, could provide a leading indicator of
performance in presidential polling. Specifically, they propose a
sentiment measure $x_w$ that for a set of topic words $w \in W$:

\begin{equation}
  \label{eq:2}
  x_{t,w} = \frac{\textrm{count}_t(\textrm{pos. word} \wedge w)}{\textrm{count}_t(\textrm{neg. word} \wedge w)}
\end{equation}

For topic words like [\textit{jobs}, \textit{economy}, \textit{job}]
(economic conditions) and [\textit{obama}] (presidential approval),
they show that these sentiment indices could provide leading
correlations for consumer sentiment or presidential approval
surveys. But both the correlations and their performance as predictor
variables was unstable over time. Likewise, simple message volume
measures--what \citep{digrazia2013} later termed the ``all news is
good news'' model--were good predictors of Barack Obama's poll
performance in 2008, but not John McCain's. Indeed, more mentions of
McCain were correlated with better performance for Obama, suggesting
that mentions were proxying for something entirely unrelated
to political sentiment. Given these
outcomes, the authors caution against relying on social media for
forward-looking performance.

Other forecasts were less circumspect. \cite{tumasjan2010election}
claimed to have successfully forecast the 2009 German elections using
Twitter data. They conclude that ``the mere number of tweets
mentioning a political party can be considered a plausible reflection
of the vote share and its predictive power even comes close to
traditional election polls.'' This is surprising, given their
admission elsewhere in the paper that a handful of heavy users tend to
dominate the production of Twitter content. 

These claims did not hold up to scrutiny.
\cite{jungherr2012pirate} show that the apparent success was an
artifact of researcher choices rather than research design. Rectifying
those errors generated predictions that the Pirate Party--a niche
party with a technologically-attuned constituency--would win. Recent
attempts to apply their methods to the 2013 German election likewise
forecast a win for the Pirate Party, in an election won decisively by
the Christian Democrats.

\cite{bermingham2011using} built forecasts from a sentiment analysis
of tweets and a comparison of candidate's relative message
volumes. Their results under-performed compared to conventional
polling in the 2011 Republic of Ireland general
elections. \cite{sang2012predicting} faired somewhat better in the
2011 Dutch elections using similar methods. However, both results relied on ad-hoc
re-weighting of Twitter forecasts using polling information. This
reliance on traditional polls casts doubt on whether it was the
Twitter data and algorithms or the poll that did the heavy lifting.

Finally, volume-based forecasts have also proven popular. The ``one
tweet one vote'' heuristic simply assumes that greater attention to a
candidate on Twitter correlates with a higher likelihood of electoral
success. \cite{digrazia2013} suggested that metrics based on message
volume alone could contribute value to forward-looking election
predictions in U.S. House races. However, \cite{huberty2013twitter}
shows that such volume metrics do not contribute predictive value above
and beyond a candidate's prior vote share when employed in true
forward-looking electoral forecasts. The complexity of political
speech--particularly on Twitter, where the 140 character limit leads
to very unique grammar--should throw doubt on the ability of such
simple metrics to tease out real voter sentiment or
intent.\footnote{\cite{owoputi2013improved} illustrate the need for
  highly customized natural language tools to properly understand the
  grammatical structure of Twitter speech. The use of abbreviation and
  emoticons, and the blurring of traditional sentence structure,
  complicate traditional part-of-speech tagging.}

Apart from the empirical and methodological shortcomings of past
electoral forecasts, the data source itself may threaten the validity
of most of these estimates. Forecasts based on data scraped from
Twitter's Streaming API mostly rely on the freely available 1\% sample
stream. Access to the Gardenhose (10\%) and Firehose (100\%) samples
is expensive, harder to arrange, and computationally more intensive to
handle. But as \cite{morstatter2013sample} show, the 1\% sample is not
a valid random sample of the Twitter message feed. As such, even if
the Twitter message stream were a representative flow of information
about the electorate, the most common form of access to that stream is
not. Instead, it further skews an already skewed signal of political
sentiment.

\cite{metaxas2011not} note that most of these attempts all suffer from
an additional problem: they all claim to predict, but in reality
back-cast. Each of these papers adopt a similar approach to model
building and evaluation: for a given election, split the set of
outcomes into training and testing datasets, holding back the testing
set to evaluate model performance. This is emphatically not the same
thing as forecasting a future election. The terms of contestation, the
issues at stake, and the language environment all change significantly
from election to election. Political actors encounter powerful
incentives for both electoral innovation and strategic behavior. In
this environment, evaluating models within the framework of a single
election does not provide a sufficient test of model performance. Only
true forward-looking forecasts of future elections suffice.


\section{A multi-cycle example of failure}
\label{sec:multi-cycle-example}

%% Here, the Twitter stuff

The author can add his own experience to the catalog of failed
attempts at election prediction. Over the 2010-2012 election cycle, we
performed what we believe to be the first multi-cycle experiment
forecasting elections in the U.S. House of Representatives. The
sampling strategy was designed to circumvent some of the problems
inherent in using the Streaming API. We made and disclosed forecasts
for the 2012 House elections in real time during the campaign, based
on algorithms and data handling methods disclosed ahead of time. Hence
our approach constituted a true out-of-sample, transparent, forward-looking attempt
at election forecasting from social media. However, despite evidence
from 2010 that our approach could beat incumbency as a predictor of
electoral success, our methods did no better than incumbency in 2012.

Here we present a post-mortem of the project. Post-hoc examination of
the 2010 and 2012 data show how the Twitter userspace and terms of
political conversation changed over the period in question. This
occurred even for an electoral system with very short periodicity
between elections, and for two elections that were fought on similar
national political issues and cleavages. Despite these favorable
conditions, both social media dynamics and electoral conditions
changed sufficiently as to quickly erode the utility of algorithms
trained from historical data. 

\subsection{Design and data gathering}
\label{sec:design-data-gath}

This experiment evolved from an attempt to address some, though by no means
all, of the problems outlined in section \ref{sec:univ-fail-elect}. We describe an open, fully out-of-sample test
of a prediction algorithm grounded in a narrow assumption about the
behavior of political language. This test is intended to address the
problems of transparency, back-casting, and user base instability that
have plagued past election predictions.\footnote{All forecasts reported here were published in
real time at \texttt{http://californianewsservice.org/category/tweet-vote/}.  All
algorithms and other code are available at
\texttt{https://github.com/markhuberty/twitter\_election2012}. We note one departure from this commitment to
  pre-release: examination of predictions post-hoc showed that an
  off-by-one error in one step of the cleaning process led to
  mis-alignment of data with districts about halfway through the general
  campaign. This led to a spurious collapse in predictor accuracy. The
  source of the error remains unclear. Fixing this error resolved the
  accuracy issue. The actual prediction algorithms used remained
  unchanged.} 
The differences between the 2010 and 2012 elections make this
particular experiment a very strong test. Three differences in
particular stand out: the shift from a midterm to a Presidential year
election; the post-2010 redistricting; and the pronounced 2010
Republican swing. Each of these three differences should challenge the
portability of an estimator across election cycles.

In practice, a strong estimator compliant with the critiques outlined
by \cite{metaxas2011not} would take the following form:

\begin{equation}
  \label{eq:1}
  V_{c,t} = V_{c,t - 1} + X_{c,t}
\end{equation}

Where the vote share received by a candidate $c$ in an election held
at time $t$ would be forecast by the vote that they received in the
prior election at $t-1$, and some set of additional predictors $X$ (in
this case derived from social media data). In the case of US
elections, $V$ may be abstracted to the party vote for an electoral
district, consistent with observed party incumbency effects that
obtain even when a given incumbent retires. Plausible predictors
should thus model (1) structural advantages that accrue to incumbents
across elections, and (2) the election-specific effects that may or
may not displace the incumbent. Estimators are only of serious
interest if the adjustments $X$ off the incumbency baseline $V$
improve the overall predictive rate of the estimator.

The estimator presented here implicitly separates these two
effects. We employ statistical machine learning methods to model the
relationship of word \textit{n-grams} in the Twitter feed for
individual candidates to their election outcomes. Observation of the
components of the resulting algorithm shows that influential terms
fall into 2 classes:

\begin{enumerate}
\item Indicators of the incumbent party
\item Salient issues and / or candidate sentiment
\end{enumerate}

In other words, the algorithm derives from Twitter language itself the
incumbency signal. It then weights other terms to adjust from that
incumbency baseline. In doing so, it embeds the following hypothesis
about future elections: that given sufficiently short election cycles,
the salient issues and patterns of political language will change
relatively little. If that hypothesis holds, then the
algorithm--nothing more than a map from the Twitter convenience sample
to the behavior of real-world voters--may offer reasonable predictive
performance.

\subsection{Data acquisition and cleaning}
\label{sec:data-acquisition}

Data acquisition used the Twitter search API\footnote{The Search API
  is subject to different restrictions than the Streaming API whose
  biases \cite{morstatter2013sample} document. Search restricts access
to the most recent 1500 messages returned by a given query. Most
candidates averaged less than 1500 messages per day. Hence it appears
reasonable to assume we have the population of candidate mentions for
most candidates. But this assumption does not hold for high-profile
candidates like Nancy Pelosi or Paul Ryan, who may receive ten times
that number of mentions on a daily basis.} to return messages
mentioning the full name of Democratic or Congressional candidates
running in contested races.\footnote{Data acquisition began on September 12
2010; and on September 1, 2012. Dates were timed to fall after as many
primary elections as possible.} We queried the API nightly during the
general election campaigns in 2010 and 2012. Table
\ref{tab:volume-by-party-inc} provides summary statistics on total
message volumes per candidate. As is immediately apparent, the observed
Twitter volume per candidate exploded between 2010 and 2012. The query
procedure generated approximately 260,000 messages for 313 districts
in 2010, the same procedure collected 1.3 million messages for 369
districts in 2012.  %% Double check this count (esp districts)

Data cleaning retained only data for districts wherein both candidates
had Twitter content. Further data cleaning removed extraneous messages
resulting from name homonyms, and excluded obviously irrelevant
non-political data. Extraneous content was identified by modeling
tweet content using a Latent Dirichlet Allocation topic model
\citep{blei2003latent} applied to ``documents'' created by aggregating
all tweets relevant to each district. Tweets were subsequently
excluded based on terms discovered via those
models.\footnote{Football- and baseball-related messages were common
sources of irrelevant data. In 2010, one candidate shared a name with
the kicker for the New Orleans Saints. In 2012, a candidate for
California District 34 shared a name with a prominent sports
journalist. Terms used for message filtering were thus dominated by
sports-related works like \texttt{mlb},
\texttt{yankees}, and \texttt{saints}.} Data
cleaning reduced the overall message volume by approximately 10\% in
2010 and 15\% in 2012.

The cleaned data demonstrate the first significant result presented in
this paper: Twitter volumes are strongly biased in favor of
incumbents. As figure \ref{fig:cand-msg-volume} shows, incumbents
received significantly greater attention on Twitter than challengers.
The detailed breakdown in table \ref{tab:volume-by-party-inc} shows
that a Democratic incumbent received approximately 33\% more messages
than their Republican challenger in 2010; and nearly three times more
in 2012. Similar results obtain for Republican incumbents. This
imbalance suggests why volume-based forecasting algorithms (e.g.,
\cite{digrazia2013,tumasjan2010election,bermingham2011using}) work:
candidates' message volumes echo an ingrained bias towards incumbents
that manifests itself across a variety of measures (fund raising,
conventional media attention, name familiarity, etc), and which
correlates well with high incumbent rates of success. 

% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=\textwidth]{../figures/plot_daily_party_volume}
%   \caption{This figure shows the daily aggregate message volume by 
% party for each day of data gathering up to Election Day 2012.}
%   \label{fig:daily-msg-volume}
% \end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{../figures/plot_raw_cand_volumes}
  \caption{Incumbents receive far more attention from Twitter users
    than challengers. This figure shows the comparative message volume for
    candidate pairs in each district. Colors indicate the party of the
    district incumbent. The diagonal line illustrates where points would fall if both candidates in a district received equal message volume.}
  \label{fig:cand-msg-volume}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{../figures/plot_msg_volume_vote_spread}
  \caption{Competitive districts receive more attention on
    Twitter. This figure shows how total district message volume
    varies with district competitiveness. Elections decided around the
  50\% cut point receive orders of magnitude more attention from
  Twitter users than less competitive races.}
  \label{fig:msg-volume-vote-spread}
\end{figure}

\input{../tables/tab_volume_by_party_incumbency}

\subsection{Data transformation for prediction}
\label{sec:data-transf-pred}

For use in prediction, the data were transformed into a vector-space
representation of text in the following steps:
\begin{enumerate}
\item URLs, punctuation, English stopwords, and non-ASCII characters were removed and
  message case was standardized
\item Candidate names were replaced with party-specific placeholders
\item Prominent officeholder names (the Speaker of the House, the Senate
  Majority Leader, and the President) were replaced with
  office-specific placeholders. 
\item A message-level term-frequency matrix
  was consolidated into a district-level matrix by summing term
  frequencies for each row belonging to a distinct district. In 2010,
  terms were restricted to those appearing in more than 1\%, and less
  than 99\% of districts. The resulting vocabulary was then used to build
  the 2012 prediction matrices. Message-level term frequencies were
  weighted by creation date prior to summation such that messages
  occurring closer to election day received higher weights. Win-loss data were
   weighted linearly relative to election
  day. Vote share 
  predictors used a uniform weight. These weights were
  selected based on their relative performance in out-of-sample tests
  on the 2010 data. The district-level term-frequency matrix was row-normalized.
\end{enumerate}

\subsection{Prediction algorithms}
\label{sec:pred-algor}

Prediction algorithms were developed based on 313 districts from the 2010 House of
Representatives election. An ensemble machine learning
algorithm \citep{van2007super} was trained on a random 90\% subset of
these districts, using actual election outcomes. Separate prediction
algorithms were designed to forecast binary win/loss and Democratic
vote share outcomes.\footnote{For binary win/loss
  prediction, the ensemble
included variants on: lasso, support vector machines with various
kernel parameters, and random forests with various tuning
parameters. For vote share prediction, this ensemble was expanded to
include boosted regression, sparse partial least squares, step
regression, ridge regression, and multivariate adaptive splines. Algorithms were selected for their capacity to deal with
high-dimensional, very sparse data.} Accuracy was
estimated against the held-out districts. Estimates showed that binary
prediction did best when term frequencies received uniform time
weights; and that vote share prediction did best when term frequencies
received weights inverse linear in the number of days prior to
election date. These algorithms and the dictionaries of their
predictor terms were then used to generate forecasts for each day of
the 2012 general election campaign.

\subsection{Results}
\label{sec:results}

We discuss four results:
\begin{enumerate}
\item Election predictions were reasonably accurate given the relative
  simplicity of the predictors. This accuracy persisted even for
  estimates made far in advance of the election. But no
  forward-looking forecasts could beat incumbency as a simple
  heuristic for electoral success.
\item Examination of algorithm performance shows that predictions are
  made in reference to terms signaling the party of the incumbent; and
  then adjusted from that basis. This presents the danger of
  algorithms regressing to the incumbency baseline over time, as was observed.
\item User and message content displays significant partisan biases
  that do not align with the general electorate and are unstable over time.
  This may render Twitter messages suspect as a means of measuring
  social sentiment
\item Partisan communities in Twitter display significant partisan
  homophily, consistent with results obtained under very different
  sampling and partisan ranking assumptions. 
\end{enumerate}

\subsubsection{Vote share predictions}
\label{sec:predictions}

Figure \ref{fig:prediction-corr-bydate} summarizes the predictive
accuracy for both vote share and win/loss predictions. 2010 results
back-casted election outcomes correctly at rates exceeding the
rate of incumbent success in this sample. For 2012, some regression to
the incumbent win rate was observed: both the win-loss and vote share
algorithms forecast the correct winner in 88\% of cases, approximating
the 90\% rate of incumbent re-election for districts in our sample.

Much of the mis-prediction error can be attributed to systemic
predictor bias against Democratic candidates. As figure
\ref{fig:corr-voteshare} shows, the vote share predictor
under-predicted Democratic vote shares in a large number of districts
that the Democrats ultimately won. In contrast, vote shares in districts
won by Republicans were forecast much more accurately. Table
\ref{tab:accuracy-by-incumbency} makes this clear for the overall
point estimates: Republican districts were successfully forecast at
rates equalling the incumbent success rate, while forecasts for
Democratic districts ran well behind the incumbency baseline. This outcome
confirms a concern about Republican bias in the training algorithm: to
the extent that the algorithm learned an implicit bias from the 2010
elections, in which the Republicans achieved a historic swing in party
control of House seats, it would carry this bias forward. This problem
may diminish as future elections contribute to the pool of training data.



\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{../figures/plot_corr_final_prediction_actual}
  \caption{Predicted vote shares correlate well with actual
    outcomes. This figure shows the correlation between predicted and
    actual vote shares for the 2010 and 2012 elections. District
    labels are colored according to the party of the electoral winner.}
  \label{fig:corr-voteshare}
\end{figure}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{../figures/voteshare_winloss_correlation_bydate}
  \caption{Predictive algorithms provide accuracy leading
    forecasts. This figure shows the predictive accuracy of both the
    vote share and win/loss algorithms for each day in the general election campaign. Predictions are back-cast for the 2010 election, using the trained algorithm; and forecast for the 2012 election. Vote shares were converted to win/loss predictions at the 50\% cut point. Horizontal lines indicate the incumbent win rate for the districts in the total population of forecasted districts.}
  \label{fig:prediction-corr-bydate}
\end{figure*}

\input{../tables/predictive_accuracy_election_incumbent}


% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=0.9\columnwidth]{../figures/plot_accuracy_by_quantile}
%   \caption{Predictive accuracy is biased towards Republican
%     candidates. This figure illustrates how predictor performance
%     breaks down according to the party of the district incumbent, and
%     the competitiveness of the race. Notice that predictions for
%     Democratic districts where Republicans won far outperformed
%     Republican districts where Democrats won. Competitiveness is defined according to the ultimate Democratic vote share in the district. }
%   \label{fig:accuracy-by-competitiveness}
% \end{figure}

% \begin{figure}[ht]
%   \centering
%   \includegraphics[height=0.3\textheight]{../figures/plot_ae_by_quantile}
%   \caption{Vote share forecast errors are worse for close
%     districts. This figure shows the absolute vote share prediction
%     error broken down by
%     district incumbent and election competitiveness. Forecast error is
%     scaled by the margin of difference between the actual vote outcome
%     and 50\%. Competitiveness
%     is defined according to the ultimate Democratic vote share in
%     the district.}
%   \label{fig:mae-by-competitiveness}
% \end{figure}


\subsubsection{Algorithms rely on incumbent cues as a baseline}
\label{sec:algor-behav-perf}

Results from multi-cycle forecasting thus evidence regression to
the incumbency baseline. Examination of the structure of the
forecasting algorithms suggests why. The use of
bigram algorithms permits partisan and officeholding cues to exist in
the same features. As figure \ref{fig:rf-term-importance} shows, those
features receive substantial weight in the random forest components of
the ensemble predictor. Additional terms then adjust from this incumbency baseline. The
importance of these linguistic cues in the predictor is confirmed by
attempting to re-train a predictive algorithm on data pre-cleaned to
eliminate these cues; predictive accuracy, even for back-casting,
declines significantly.

This is not, per se, a bad strategy: the high rate of incumbent
re-election in the US system makes the incumbency baseline ideal. But
the instability in salient political issues makes consistent
adjustment from that baseline difficult. For instance, ``4 hcr''
(``for health care reform'') was a
top predictive term for the vote share predictor. The health care debate played a
significant role in the 2010 elections, which followed closely on the
passage of the Patient Protection and Affordable Care Act, otherwise
called Obamacare. Two years later, however, jobs and fiscal policy
played a much more salient role. Unstable issue salience complicates
attempts at topic-driven forecasting such as was attempted
here. Moreover, what counts as issue salience for the
politically-active Twitter population may not prove salient for the
broader electorate. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{../figures/plot_rf_algorithm_term_importance}
  \caption{Prediction algorithms key in on incumbency
    indicators. This figure shows the estimated term importance for the random forest algorithm component
    of the vote share and win-loss ensembles. Term importance is
    estimated as the normalized change in predictive error upon
    random permutation of each term. Each panel shows the top
    30 terms by importance for the highest-weighted random forest
    member of the SuperLearner library.}
  \label{fig:rf-term-importance}
\end{figure}

\subsubsection{Twitter users are highly partisan}
\label{sec:part-cont-twitt}

Mapping from Twitter conversations to election outcomes presumes not
just linguistic stability, but also some stability in Twitter's
convenience sample of the electorate. To investigate the dynamics of the
politically-engaged Twitter community further, we would like to know
both (1) the pattern of partisan alignment of users in our sample and
(2) patterns of communication among partisans. We estimate the
partisan alignment of Twitter users via a two-step approach. First, we
identify a set of Twitter hashtags with a known partisan alignment. In
this case, we choose \texttt{\#tcot} for conservatives and
\texttt{\#p2} for liberals. Following \cite{conover2011}, we then
index all hashtag-containing messages by their hashtags. Using this
index, we compute the Jaccard similarity for co-occurrence of each
unique hashtag with the known partisan tag. We retain separate lists
of conservative and liberal tags with a Jaccard index of greater than
0.01. We make these lists exclusive by discarding any confusion terms
appearing in both lists.

Based on these hashtags, we construct a partisan score, $p_u$ for each
user that employed hashtags in their messages. The scores is computed
as the scaled difference between their use of conservative and liberal
tags, on the interval $[-1, 1]$ from most liberal to most
conservative. Formally:

 \begin{equation}
   \label{eq:pscore}
   p_u = \frac{\left|tags_{u,cons}\right| - \left|tags_{u, lib}\right|}{\left|tags_u\right|}
 \end{equation}

To score those users that do not use hashtags, we use the scored users
as an input to train a classifier based on the text of user
messages. Formally, we convert the continuous partisan score to binary
``liberal'' / ``conservative'' score around a cut point at zero. We
then train a naive Bayesian classifier to predict a user's
partisan class based on the language of their aggregated
messages.\footnote{Formally, we aggregate each user's tweets and
transform them into a term-frequency matrix with one row per
user. Term frequencies are TfIdf-weighted. Hashtags,
common English stopwords and terms used by fewer than 0.1\%, or more
than 99.9\%, of users were discarded. URLs were standardized to a
'URL' placeholder. All computation used the \texttt{sklearn} machine
learning suite for Python \citep{scikit-learn}.} Cross-validated
predictive accuracy averaged 92\% for the 2010 user population, and
86\% for the 2012 population. We then use the trained classifier to
predict the partisan alignment of all users, based on the text of
their messages.

These estimates raise two issues about the nature of the Twitter
user population and its utility as a sample for real-world political
forecasting. First, by this measure Republicans on Twitter are more
strongly partisan-aligned than Democrats. Figure
\ref{fig:user-pscore-distribution} shows that conservative-leaning
users score as more extreme partisans than liberal-leaning users.

Second, conservative-aligned users are better represented in the
population than liberals. Conservatives outnumbered liberals 6.8:1 in
2010, and 2.2:1 in 2012. This Republican dominance of
politically-active Twitter users contrasts with the general electorate,
in which the share of self-described Democrats has remained stable as
the percentage of self-described Republicans has fallen since 2000
\cite{pew2012}.



\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{../figures/user_partisan_alignment_density}
  \caption{Twitter users are highly partisan. This figure shows the distribution of partisan alignment
    scores for users whose tweets contain hashtags. Scores were
    calculated on the $[-1 , 1]$ interval, where 1 is most
    conservative. Liberal scores are shows as absolute values to permit direct comparison.}
  \label{fig:user-pscore-distribution}
\end{figure}



% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=\textwidth]{../figures/mt_graph_path_length_distribution}
%   \caption{Path length distribution between co- and anti-partisans within the \textbf{mention} graph. Path lengths are computed as the Dijkstra shortest path between a 1,000 random liberal-liberal, conservative-conservative, or liberal-conservative node pairs. }
%   \label{fig:rt-partisan-path-length}
% \end{figure}

\subsubsection{Twitter users display significant partisan homophily}
\label{sec:twitt-users-displ}

Third, Twitter communication networks display very high partisan
homophily, even if the content of the communication spans partisan
boundaries. We use two forms of communication to detect partisan
homophily. First, one user may \textit{retweet}, or forward, messages
from another. Second, one user may \textit{mention} another in a
message. Since retweets require a user to first have received the
message being retweeted, they tend to signal communication
links. Mentions, in contrast, may signal only attempts at
communication or citation. To put this another way, retweets may
signal conversation, while mentions signal references.

We measure partisan homophily through the communication graph created
by users either retweeting or mentioning one another's messages. The
graph in this case is constructed of users (nodes) connected by either
retweet or mention edges. Quantitative measures of the connectedness
of this graph suggest a high degree of partisan homophily: individuals
talk to their co-partisans, but talk about their anti-partisans.

We measure this in two ways. First, we exploit graph connectedness to
determine how closely bound partisan types are to each other and to
their anti-partisans. Three connection types are possible: two
co-partisan (Liberal-Liberal and Conservative-Conservative) and one
anti-partisan (Liberal-Conservative). For each type, we compute the
Dijkstra shortest path for 1000 randomly chosen pairs of each
type. The resulting mean path lengths, show in figure
\ref{fig:path-lengths}, suggest a world in which individuals are far
more likely to be connected to, and to converse with, their
co-partisans, even if they may talk about both co- and anti-partisans.

We confirm this impression by investigating the community structure of
the relationship graphs. Community detection used the Louvain
algorithm \citep{blondel2008fast}. For a community $C$ containing
users $U$ with partisanship scores $p \in \{-1, 1\}$, community partisanship was computed
as $p_C = \frac{1}{\left|U\right|}\sum_U p_u$. Figure
\ref{fig:partisan-subcommunities} 
illustrates the distribution of community partisan membership
for both the mention and retweet graphs. Figures
 illustrate the graph
structure linking these communities to each other.\footnote{Graph
  nodes correspond to single communities. Edges correspond to mentions
or retweets between community members. Node color corresponds to the
partisan mix of community members.} Again, we see that users tend
to speak to one another, while talking about a more uniformly
distributed set of partisans. The retweet graphs, signalling
communication, cluster by partisan membership; while the mention
graphs show dense connections between communities of different
partisan content. These results are consistent with
\cite{conover2011}, under completely different sampling procedures. 



% \begin{figure}[ht]
%   \centering
%   \includegraphics[angle=90, width=0.7\textheight]{../figures/user_rt_largest_cc_mst_2010.png}
%   \caption{Network representation of \textbf{retweet} connections between
%     users for \textbf{2010}. Colors represent a user's partisan alignment towards the
%     Republicans (reds) or Democrats(blue). Labels illustrate users with
%     very high degree centrality. This representation uses the maximum
%     spanning tree of the largest connected component of the retweet graph.}
%   \label{fig:rt-largest-cc-mst-2010}
% \end{figure}


% \begin{figure}[ht]
%   \centering
%   \includegraphics[angle=90, width=0.7\textheight]{../figures/user_rt_largest_cc_mst_2012.png}
%   \caption{Network representation of \textbf{retweet} connections between
%     users for \textbf{2012}. Colors represent a user's partisan alignment towards the
%     Republicans (reds) or Democrats(blue). Labels illustrate users with
%     very high degree centrality. This representation uses the maximum
%     spanning tree of the largest connected component of the retweet graph.}
%   \label{fig:rt-largest-cc-mst-2012}
% \end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{../figures/plot_rt_mt_pathlengths_2010_2012}
  \caption{Conservatives are more tightly connected to each other via
    conversation than liberals. This figure shows the 95\% confidence interval of the mean
    Dijkstra path length between co- and anti-partisans in the
    retweet and mention graphs. Path lengths were computed for 1000 randomly chosen pairs of each type.}
  \label{fig:path-lengths}
\end{figure}

% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=0.9\columnwidth]{../figures/mt_rt_graph_path_length_distribution}
%   \caption{Path length distribution between co- and anti-partisans. Path lengths are computed as the Dijkstra shortest path between a 1,000 random liberal-liberal, conservative-conservative, or liberal-conservative node pairs. }
%   \label{fig:partisan-path-length}
% \end{figure}

% \begin{figure}[ht]
%   \centering
%   \includegraphics[angle=90,
%   width=0.7\textheight]{../figures/user_mt_largest_cc_mst_2010.png}
%   \caption{Network representation of \textbf{mention} connections between
%     users for \textbf{2010}. Colors represent a user's partisan alignment towards the
%     Republicans (reds) or Democrats(blue). Labels illustrate users with
%     very high degree centrality. This representation uses the maximum
%     spanning tree of the largest connected component of the mention graph.}
%   \label{fig:mt-largest-cc-mst-2010}
% \end{figure}

% \begin{figure}[ht]
%   \centering
%   \includegraphics[angle=90,
%   width=0.7\textheight]{../figures/user_mt_largest_cc_mst_2012.png}
%   \caption{Network representation of \textbf{mention} connections between
%     users for \textbf{2012}. Colors represent a user's partisan alignment towards the
%     Republicans (reds) or Democrats(blue). Labels illustrate users with
%     very high degree centrality. This representation uses the maximum
%     spanning tree of the largest connected component of the mention graph.}
%   \label{fig:mt-largest-cc-mst-2012}
% \end{figure}


%% NOTE: notice that for the communities, there's a baseline random
%% prob of membership if the communities were just drawn from the
%% existing distribution. So bootstrap that: draw 1000 samples of k
%% pairs and figure out what the distribution of membership looks
%% like. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\columnwidth]{../figures/rt_mt_community_partisanship}
  \caption{Retweet communities display greater partisan homophily than
    mention communities. Retweet communities tend to contain
    communities with well-defined partisanship (either very liberal,
    or very conservative). Mention communities display more uniformly
    mixed partisan members.}
  \label{fig:partisan-subcommunities}
\end{figure}

% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=\textwidth]{../figures/voteshare_prediction_corr_bydate}
%   \caption{Predicted and actual vote shares by district for 2010 and 2012. For 2010, all numbers reflect back-casting using the trained algorithm. For 2012, predictions reflect the forecasted Democratic vote share on election day. }
%   \label{fig:predicted-actual-voteshare-bydistrict}
% \end{figure}

\begin{figure*}[ht]
  \centering
  \begin{minipage}[h]{0.45\linewidth}
    \begin{center}
      \includegraphics[width=\textwidth, clip=true, trim=0 2cm 0 2cm]{../figures/mt2010_community_graph}
      Mentions 2010
    \end{center}

  \end{minipage}
  \begin{minipage}[h]{0.45\linewidth}
    \begin{center}
      \includegraphics[width=\textwidth, clip=true, trim=0 2cm 0 2cm]{../figures/mt2012_community_graph}
      Mentions 2012
    \end{center}
  \end{minipage}\\
  \begin{minipage}[h]{0.45\linewidth}
    \begin{center}
      \includegraphics[width=\textwidth, clip=true, trim=0 2cm 0 1cm]{../figures/rt2010_community_graph}
      Retweets 2010
    \end{center}
  \end{minipage}
  \begin{minipage}[h]{0.45\linewidth}
    \begin{center}
      \includegraphics[width=\textwidth, clip=true, trim=0 2cm 0 1cm]{../figures/rt2012_community_graph}
      Retweets 2012
    \end{center}
  \end{minipage}
\caption{Retweet and mention patterns among partisan communities on
  Twitter. Nodes in each figure correspond to communities in the
  retweet or mention graph as discovered by the Louvain
  algorithm. Node color reflects the mean partisan makeup of the
  community. Edges are weighted by the degree of interconnectedness
  among users in each community.}
\label{}
\end{figure*}



\subsection{Accounting for failure}
\label{sec:accounting-failure-1}


The results presented here point to the difficulty of forecasting
beyond heuristics using convenience samples in social
media. Prediction algorithms that showed promise in the 2010 election
saw significant regression by the 2012 cycle. Patterns of Twitter
message volume displayed significant ingrained biases towards
incumbents. The partisan distribution of Twitter users is
significantly skewed, in ways that run contrary to the evolving
partisan structure of the American electorate. Patterns of
communication between users suggest that Twitter partisans are fairly
isolated: while they may speak of their partisan opponents, evidence
of strong connections between anti-partisans is weak. To the extent
that anti-partisans are connected in the communication graph, it is
the large media organizations, rather than individuals, who function
as bridges. Each of these phenomena evolved between the 2010
and 2012 elections, weakening the effect of whatever learned
compensation mechanisms had helped the 2010 predictions beat the
incumbency baseline. 

These factors suggest future challenges for building valid forecasting
models. The forecasting strategy here implicitly learned the
incumbency baseline and then attempted to adjust off that baseline
based on patterns of linguistic content about candidates. Similar
strategies have proven useful in other contexts, such as forecasting
macroeconomic aggregates like unemployment rates
\cite{choi2012predicting}. But the results here suggest greater
instability in the underlying map between online and offline behavior
than exists in the macroeconomic context. Furthermore, the Twitter
userbase appears more highly partisan, more isolated, and more
rancorous than the voting public at large. 

This problem is not new. Google's historically accurate search-based
influenza forecasts \citep{ginsberg2008detecting} broke down in both
2009 and 2012, consequence of changes to the epidemiological profile
of the flu virus and its coverage in the media
\citep{butler2013google,41763}. Twitter has far lower penetration in
the general populace than Google search, and greater volatility in its
user base and usage patterns. Hence we should expect even more
problematic deviations between elections than what we have seen
heretofore.

%% Here
% 1. Incumbency + 
% 2. Changing volumes
% 3. Changing patterns of partisan isolation (networks)
% 4. Anti-patterns of partisan alignment vs. general population


\section{Towards better political forecasts}
\label{sec:towards-bett-polit}

Given this pattern of failure, and the thorny root causes thereof,
what might improve the odds of building successful social media-based
forecasts of election outcomes? \cite{gayo2012wanted} attempts to
define a baseline set of criteria for serious election forecasts from
Twitter data. The recommend, at a minimum:

\begin{enumerate}
\item Forecasters should have a well-defined hypothesis for why their
  method will work
\item Forecasters should define ahead of time the right
  baseline--usually the rate of incumbent re-election--against which
  they should be judged
\item Forecasters should have well-defined means of handling spam,
  name ambiguity, humor, sarcasm, and other noise-generating processes
\item Forecasters should disclose all data acquisition, cleaning, and
  forecasting methods before the election they intend to predict
\item Forecasters should make public their predictions ahead of the election in question
\end{enumerate}

This is a very good baseline. It strongly suggests that future claims
of successful prediction with simple metrics like na\"ive sentiment
scoring or message volume should be treated with extreme skepticism,
given their thin justification for a plausible relationship between
Twitter behavior and population-wide political outcomes. It also
informs against the hope that Twitter would provide a more direct way
to measure voter sentiment than polling. Instead, all experience to
date shows that Twitter (and likely other social media services) will
require significantly more work to account for the difficulties of
sampling, language, and behavior. Each of these difficulties occurred
in the past for polling as well: the entire edifice of modern sampling
methods reflects a long process of learning to account for
them. Forecasting complex events from social media will require no
less.
 
Even with this baseline, however, data remains a problem. Sampling
from the Streaming API appears both inadequate in practice and
potentially non-reproducible between similar analyses. Twitter's terms
of service prohibit data sharing without explicit permission. Hence
disclosure of methods and forecasts is necessary but insufficient. Use
of the Search API appears to mitigate the sampling problem, though to
do so researchers must be querying the database at roughly the same
time. Data sharing will require explicit arrangements with
Twitter. For elections of sufficient interest--notably national
elections in the United States--scholars may do well to negotiate with
Twitter for access to a public dataset.


\subsection{Beyond transparency and commitment: dynamic validation in
  an unstable world}
\label{sec:dynamic-validation}

Even if these problems are overcome, however, one significant problem
remains. We can abstract all the difficulties encountered by the
papers presented in sections \ref{sec:univ-fail-elect} and
\ref{sec:multi-cycle-example} as a validation problem. Given the
selection and behavior problems inherent in the Twitter sample,
predictors will function by finding a suitable map from the Twitter's
convenience sample of online expression to the behavior of the offline
population. That map could be simple and explicit--the direct
connection of one mention with one vote--or complex, as in the machine
learning-based re-weighting scheme employed by the author.

Regardless of how the map is drawn, forecasting requires that remain
valid until the next election. Whether it is depends on how quickly
its implicit assumptions about the relationship of the online world to
the offline degrade. As we've seen, that can happen quite
rapidly. This means that any valid forecasting method will need some
means of dynamically checking and updating its assumptions in response
to changes in the online or offline worlds. We can abstract those
changes into two categories: ephemeral changes in political salience
or how political topics are expressed on Twitter; and structural
changes in the rules that govern either political outcomes or online
behavior. Dynamically updating models to account for these changes
poses serious challenges.

\subsubsection{Politics is not advertising: accounting for evolving changes in the political}
\label{sec:acco-evolv-chang}

Past successes in forecasting offline behavior from online data have
generally occurred in areas where individual events are plausibly
exchangeable, and incentives and opportunities for strategic behavior
are very limited: individual purchasing decisions for standardized
goods; individual flu infections; or individual responses to highly
formalized advertising. Each of these situations enjoyed very large
amounts of data about high-frequency and atomistic human actions and
responses. Consequently, the cycle of model building and checking was
short, and the opportunities for experimentation across large numbers
of plausibly similar events abundant. Finally, the consequences of
getting any individual prediction wrong were low, so long as the
aggregate accuracy was high. These qualities made real-time updating
of forecasting models straightforward.

Elections are clearly not like this. The volatility of issue salience,
party competition, the rules that govern electoral systems, and other
factors all make individual elections highly variable. Furthermore,
elections generate enormous incentives for strategic behavior on the
part of partisans, that may well extend into how they attempt to
influence online sentiment. Finally, elections of a given type are
relatively rare, and each event is highly consequential. Together,
these changes in the dynamics of offline behavior combine to make it
difficult to generate, check, and update valid models of election
outcomes. They further suggest the need for plausible forecasting
models to dynamically check assumptions about these variables in real
time. 

It's important to note that polling does not face nearly so severe a
problem. Polling, by controlling both sampling and question framing,
retains control over much of the data-generating process. The general
framework of random sampling remains valid independent of the evolving
political context. But algorithmic forecasting from social media only
knows what it has learned from the data--data that by definition
originated from past elections in different political contexts. Hence
algorithmic forecasting will encounter greater difficulties when faced
with the same underlying political instability.


\subsubsection{Accounting for evolving changes online}
\label{sec:acco-evolv-chang-1}

If there is little reason to expect consistency in the offline world,
there's even less reason to do so online. We identify four major
problems. First, the user base for social media services is highly
unstable. Rapid growth and customer churn will very quickly alter the
profile of its users. Given that a valid forecasting algorithm
implicitly weights online expressions of opinion to account for
selection and sampling bias versus the real world, this churn will
degrade the accuracy of those weights over time.

Second, a dynamically changing user base will also affect user
behavior. Twitter, Facebook, and other online media are social
communities. Changes to the community affect the norms and
expectations by which community members behave. Exactly how, though,
is unclear \textit{a priori}. Consider, for instance, a stylized world
in which Twitter was used only by Democrats early on. In that world,
does rapid growth in the Republican user base expose Twitter's older
community to broader views, fracturing what had been a closed world?
Or does it encourage epistemic closure, effectively creating two
communities within one service? Evidence presented in section
\ref{sec:twitt-users-displ} suggests that Twitter now reflects the
latter. But early hopes for Twitter (and the internet more broadly)
emphasized the former. 

Third, the relation between online and offline salience and sentiment
may be very unpredictable. \cite{Muchnik09082013} show, in a
randomized experiment, that online ratings systems are prone to
long-term bias consequence of early ratings. Ratings for goods or
services that receive early positive reviews tend to skew positively,
and vice-versa. In the offline world, this is often called
agenda-setting \citep{scheufele2007framing,scheufele2000agenda},
wherein those who control the early terms of debate shape the entire
debate. There's little reason to believe that the pattern of online
agenda-setting will evolve in ways consistent with the offline. As
such, online sentiment towards specific politicians or policy issues
may diverge from the population sentiment in unpredictable ways.

Finally, the rules under which users operate will change. Twitter and
other social media aren't just megaphones for individual
opinion. Rather, these services are also a system of social relations
operating under a set of rules embedded in the design and operation of
the underlying software. As \cite{lessig1999code} pointed out in a
different context, these rules are as binding as anything in the
offline world. Hence even if we hold everything else constant--the
users, their political alignment, the incentives they face for getting
noticed, the structure of the communities they belong to and the norms
those communities create--changes to these rules will therefore cause
general shifts in patterns of user behavior. Evidence points to
frequent, if subtle, rule changes at companies like Facebook, where
real-time testing leads to multiple versions of the same site running
in parallel \citep{vance2012facebook}. Other, more major, rule changes
due to internal or external policy shifts are also
common. Anticipating how such changes will shape the formation of
online political sentiment and interaction poses a very tricky
challenge. 

\subsection{Towards dynamic validation of forecasting methods}
\label{sec:towards-dynam-valid}

To date, little work on political forecasting has attempted to
dynamically check the validity of a method's map between the online
and offline worlds. Nevertheless, some emerging work suggests how it
might be done. \cite{barbera2012birds} uses ideal point estimates
\citep{poole1985spatial} of Twitter-using politicians as a benchmark
for predicting the political alignment of Twitter users. Since ideal
point estimates are easily updated for each new political season, this
work provides a possible means of updating partisan forecasts without
resorting to polling.

Similarly, TweetCast (\url{http://tweetcast.knightlab.com/}) and
similar projects attempted to crowd-source updates to predictive
models of voting by Twitter users. Starting from a baseline model,
TweetCast presented users with their predicted vote and asked them to
confirm the prediction. In doing so, it built a real-time training set
specific to the 2012 election, again without resorting to polling. 

\cite{beauchamp2013} takes an entirely different approach. Rather than
attempt to forecast elections, he uses higher-frequency polling to
build a Twitter-based model of presidential candidate
sentiment. Forward-looking predictions then confirm the ability to
make accurate leading forecasts of state-level opinion surveys. This
approach does not, of course, eliminate the need for polling. But it
suggest what might be possible given more regular means of updating
than election outcomes permit.

Other possibilities surely exist. But we note that both these examples
still restricted their purview to the community of politically-active
Twitter users. While these methods help resolve the difficulty of
context-specific understanding of political language in tweets, they
do not help us update models of how patterns in that language relate
to behavior in the broader population. That remains, as of
this writing, an open problem.

\section{Conclusions: on the difficulty of forecasting}
\label{sec:concl-diff-forec}

Headline success forecasting the offline world from online data have
relied on events that are atomic, exchangeable, frequent, and
low-risk. Where forecasting models have departed from these
characteristics--as with Google Flu or attempts at stock market
forecasting--they have often run into serious difficulty. The
assumption of a relatively stable link between online and offline
behavior appears to work reasonably well when forecasting purchasing
behavior, estimating advertising effectiveness, or identifying
potential contacts. Beyond those cases, it appears less reliable.

Elections have proven no different. The record of failed attempts at
forecasting election outcomes from social media has generated a string
of criticisms. Those criticisms have emphasized both process and
method. Future forecasts should address these criticisms. The process
by which data are obtained, cleaned, and analyzed must be transparent
and reproducible, and forecasts should be made public in advance of
the election date. Methods need to explicitly deal with the known
problems of sarcasm, humor, spam, selection bias, and other
distortions that make human communication difficult to parse and
relate to real-world events. 

But these criticisms leave open the serious problem posed by the
instability of both online systems and offline political dynamics; and
the resulting corruption of forecasting methods that try to map
between the two. Deprived control over the data-generating process,
these forecasts must instead try to link online behavior to offline
intent. To date, no known forecasting methods have dealt with the
consequences of a world where online user communities are constantly
in flux, the rules of online communities are subject to change without
notice, and offline political phenomena evolve rapidly. Doing so will
require that we develop the means to dynamically check and update the
validity of forecasting algorithms. Exactly how to do so without
resorting to polling--the very thing that social media-based forecasts
hoped to replace--remains unclear. But it will remain a challenge even
after we've solved the hard tasks of teaching computers to understand
and parse the complexity of human speech and political sentiment.

% \subsubsection{Performativity}
% \label{sec:performativity}

% We close with one additional, albeit speculative, concern. We may sum
% up the central challenge of forecasting from social media as one of
% control over the data generating process. Traditional polling methods,
% when the usual sampling assumptions hold, enjoy a great deal of
% control over the means by which they measure public
% opinion. Conversely, forecasting from social media data depends on a
% series of modeling assumptions to account for the lack of control over
% this process. Those assumptions, as we've seen, may be dubious given
% the volatility of the underlying data generating process. But so far,
% there remains the theoretical possibility of overcoming these
% hurdles. 

% %% Why is performativity a problem:
% % 1. 

% But a

% The general performativity thesis can be articulated as a process by
% which a theory about how the world works becomes incorporated into the
% actual functioning of the world. \cite{mackenzie2006engine} describes
% such a process occurring in finance. There, innovations like the
% Black-Scholes pricing model quickly moved from being models of the
% state of the world to models used to price real options on real
% markets. Unsurprisingly, those markets then soon started to endogenize
% those models in their behavior. What had been models of the world were
% now the world itself. By extension, those models weren't disinterested
% observers--instead, their results comported with the world because
% they had made the world. 

% It seems likely that Twitter and its fellow-travelers may behave
% similarly.\footnote{Much of what follows owes initial inspiration to
%   heretofore unpublished work by Kieran Healy of Duke University.}
% Recall that Twitter's boosters took it to be a primarily expressive
% medium. Individuals broadcast their thoughts to a potentially
% infinite, though in practice perhaps quite paltry,
% audience. Regardless of the size of the audience, though, it's the
% thought that counts--the potentially unvarnished, if also somewhat
% unstructured, window onto the true desires and intents of voters

% In parallel, though, Twitter is a social network, through which users
% communicate primarily with their followers and only secondarily with
% the diffuse Twitter population as a whole. That communication takes
% the form of conversation and self-promotion as well as pure
% expression. Hence Twitter is no mere megaphone. Rather, its users may
% consciously or unconsciously shape what they say to reflect the
% incentives and underlying network structure in which they operate. 

% This poses several problems for online forecasting from offline data. 
% Does this matter to online forecasting? Yes, in several ways. First,
% it undercuts the thesis that Twitter, among other sources of online
% data, provides a window onto human expression. Rather, on Twitter we
% find individuals behaving as they are incentivized to
% behave online. 

% Those incentives may be set so early on that we have
% little chance to observe individuals' true offline selves. 
% Consider someone who might be classed a mild liberal, expressing their discontent
% with some far-right and perhaps marginal position. This attracts two
% forms of attention this individual might never have encountered
% previously: vitriol from far-removed right-wingers; and sympathy
% from people far to their left who they might, under other
% circumstances, have thought daft. The dynamics of Twitter, given its
% reach and openness, have now embedded an otherwise moderate person
% in a network of both enthusiastic support and vitriolic condemnation
% that might never have occurred otherwise. Observing the individual's
% Twitter network, their patterns of partisan interaction, and the
% partisanship of their followers, we might conclude that this
% individual had become more partisan over time. But of course we have
% no way of knowing that. This individual's perceived partisan shift is
% entirely a function of communication dynamics within Twitter's social
% graph. 

% Need to be crisper here: one thing to say "we have real opinions,
% just a weird sample". Another to say "we have skewed opinions in a
% skewed sample, and the skewness is really hard to figure out". It's
% still another thing to say that the skewness is a function of the
% medium itself--that is, it's instable.
%This raises the possibility that even if 


% Healy, "Until recently, no such data-generating substrate existed in
% most other social settings, of for many other kinds of
% interaction. This is now changing, as an increasing variety of
% social exchanges leave digital records in their wake.'' p 10

% One view of Twitter in particular takes it to be a fundamentally
% expressive medium in which individuals broadcast their thoughts to a
% potentially infinite, though likely almost nonexistent,
% audience. Another views it as a social network, in which one
% communicates primarily with one's followers, and only secondarily
% with a broader--and again potentially infinite--audience. Of course,
% it is really both. Twitter's dual-use status as a means of
% communication and self-promotion complicates the problem of parsing what role any
% given individual plays in any given tweet. But it also means that
% Twitter is no mere megaphone. Instead, as Healy suggests, people may
% consciously or unconsciously shape what they say to the incentives
% structure created by Twitter's underlying dynamics. 

% The tension comes from what Healy describes as the "retrospective
% analysis and description" of networked online systems alongside the
% construction of those same systems using those same ideas. Over
% time, the models become endogenous to the systems they purport to
% observe, as individuals take on the behavior of the model via the
% incentives it embeds in the operation of the system itself. 

% Does this matter to online forecasting? Yes, in several ways. First,
% it undercuts the thesis that Twitter, among other sources of online
% data, provides a window onto human expression. Rather, on Twitter we
% find individuals behaving as they are incentivized to
% behave--incentives that may be set very early on. Consider someone
% who might be classed a mild liberal, expressing their discontent
% with some far-right and perhaps marginal position. This attracts two
% forms of attention this individual might never have encountered
% previously: vitriol from far-removed right-wingers; and sympathy
% from people far to their left who they might, under other
% circumstances, have thought daft. The dynamics of Twitter, given its
% reach and openness, have now embedded an otherwise moderate person
% in a network of both enthusiastic support and vitriolic condemnation
% that might never have occurred otherwise. Whether this results in
% actual changes in offline behavior is hard to konw. But certainly
% the online image of this individual bears relatively little
% resemblance to the offline social context they live in, and possibly
% the process by which they choose their vote. This is a long way away
% from Twitter as an unadulterated window into the heretofore
% disguised souls of the electorate. 


% Gayo-Avello to start. Openness, spam, true out-of-sample (next
% election, not future same election) forecasts, etc. 

% But even doing that, at least two big problems remain:
% 1. Construct a map from the weird online world to the offline
% world. How do we test that such a map is still valid? How can we
% know? 
% 2. More troubling: what if the online world actually isn't much like
% the offline one in more fundamental ways. Mackenzie, Healy. 

\section{Conclusions}
\label{sec:conclusions}

% IF this is necessary. Might be nice to just terminate with "towards..."


%% OLD FOLLOWS

% \section{Prior forecasting results}
% \label{sec:prior-forec-results}

% Twitter has become a popular medium for both studying online political
% behavior. As an information-push medium,
% tweets promise an unvarnished, if also unstructured, look into
% individuals' political attitudes. However, successful predictors have
% proven ephemeral. Claims by \cite{tumasjan2010election} to have
% successfully forecast the 2009 German elections using Twitter
% data, were shown by \cite{jungherr2012pirate} to be an artifact of
% researcher choices rather than research
% design. \cite{bermingham2011using} used a hybrid approach, mixing
% sentiment analysis and relative message volumes. But their results
% under-perform conventional polling in the 2011 Republic of Ireland
% general elections. \cite{sang2012predicting} performed somewhat better
% in the 2011 Dutch elections, but their best results relied on ad-hoc
% re-weighting using polling information. 

% These problems, \cite{metaxas2011not} have argued, indicate a much
% broader problem for election prediction via social media. Given the
% demographic differences between the Twitter user base and the voting
% population, the inherent dynamism of political language and activity,
% and incentives for strategic behavior on the part of motivated
% partisans, simple heuristics are unlikely to perform reliably as
% electoral predictors. At the very least, they argue, claims to valid
% prediction should clearly articulate \textit{ex ante} the underlying
% assumptions made by the forecasting method, the approach to dealing
% with spam and other forms of information noise, and the baseline
% against which the estimate should be judged. 

% This paper offers a slightly different version of the critique
% advanced in \cite{metaxas2011not}. They argue that, in the limit,
% Twitter-based election forecasts need to converge on the random
% sampling methods used in professional polling. They then point out why
% that will be extremely difficult, given privacy issues and proprietary
% data. Alternatively, we can treat the problem slightly differently:
% the algorithmic task is to develop a stable map from the behavior of
% the Twitter convenience sample to the behavior of real-world
% voters. That map should have sufficient resolution to beat simple
% heuristics--particularly incumbency--for election outcomes. We should
% also have some idea of where that map might break down. 


%% Thought: elections aren't about sentiment. They are about
%% behavior. I can vote for someone only as a result of thinking they
%% are better than the other guy, but generally suck. Also problematic
%% because we know that most voters aren't well-informed. So are we
%% modeling individual choices (usually not, for incumbency) or trying
%% to pick up on broader trends across districts or within regions. 

%% Two possible ways to success:

%% 1. Stable algorithmic map, based on specific assumptions about
%% election behavior
%% 2. Random sampling method equivalent to professional polling

%% Note that (2) also embeds (1), due to likely voter models. That map
%% has broken down in the past. So these aren't specifically unique,
%% but (2) relies on a weaker version of this than (1). See also cell
%% phones vs landlines, etc. 

%% Models to show: (1) my own, (2) my own, plus prior vote share (just
%% use OLS to average), (3) back-casting to the 2010 election given
%% 2012 observations. 

%% What do we know:
%% 1. For 2010, Twitter predictions (post-hoc...) improve estimates
%% compared w/ prior voteshare alone
%% 2. For 2012, Twitter predictions do far worse than prior voteshare;
%% together w/ prior voteshare, they do marginally better; but they
%% still lose at winloss b/c the RMSEs are still too big relative to
%% the place that matters--right at the cutpoint.

%% Also: 2012 had high re-election rates for districts w/in 4pt margin
%% in 2010; but 2010 did not (unsurprising, those districts were the
%% subject of tea party challenges). So the estimator needed to be
%% even better than normal at the cutpoint, which is the really hard
%% thing to get right even if overall RMSE is low or accuracy high.

%% And back-predicting 2010 given a model of this form:
%% vote_2008 ~ vote_2010 + twitter_2010
%% Out-performs vote alone when applied to 2012
%% Placebo test strongly suggests that there's something weird going
%% on here. 



% \section{Discussion}
% \label{sec:discussion}



% \section{Conclusion}
% \label{sec:conclusion}

% Social media-based election forecasts have proliferated in the last
% several years. We provide what we believe to be the first multi-cycle
% test of election forecasting with Twitter. Our results point to the
% difficulty of building persistently accurate content-based forecasting
% algorithms. Underlying changes in election dynamics, issue salience,
% the Twitter user base, and patterns of Twitter use degrade the
% performance of prediction algorithms across election cycles. Moreover,
% the convenience sample of the Twitter user space is clearly unlike the
% broader electorate in both partisan alignment and polarization. While
% we might imagine learning a valid mapping from the Twitter user base
% to the preferences of the broader electorate, the underlying
% instability of that user base and its patterns of use will weaken the
% mapping over time. While commercial uses of Twitter-based forecasting
% face similar problems, they benefit from far more regular
% opportunities to update than the two- or four-year American election
% cycle. Likewise, while poll-of-polls based forecasts also must update
% over time, they benefit from a far larger set of opportunities to
% learn the relationship between polling aggregates and house effects on
% the one hand, and election outcomes on the other, than
% the history of Twitter or other social media permit.

% Hence building effective and informative social media-based election
% forecasting approaches will require a sustained commitment on par with
% that required to build effective forecasting and turnout models from
% polling. Obvious needs include better models of how Twitter behavior
% relates to real-world phenoma like partisan identification and
% turnout; better monitoring of how the underlying Twitter population
% evolves between elections; and better measurement of the gap between
% the politically-active Twitter population and the broader
% electorate. 


% % Here, something about how Twitter partisans are probably very likely
% % to vote, etc, and so unlike the rest of the electorate. 


\bibliography{/home/markhuberty/bibs/twitter}
\bibliographystyle{apalike}
\end{document}
